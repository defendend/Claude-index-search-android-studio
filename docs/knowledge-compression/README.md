# Knowledge Compression Engine

> Максимум релевантной информации в минимум токенов

## Проблема

AI-агенты тратят огромную квоту на работу с большими объёмами данных:
- RAG находит документы, но пихает в контекст килотонны текста
- GraphRAG лучше по связям, но всё равно дорогой по токенам
- Agentic RAG крутой, но не экономит квоту — там контекстное окно рулит

**Ключевой инсайт:** время работы не влияет на квоту, влияет только объём токенов в контексте.

## Цель

Инструмент, который **заранее** (офлайн, дёшево) превращает терабайты знаний в компактный **граф фактов**. AI работает с фактами, а не с сырыми документами.

```
Сырые данные → [Дешёвая модель + правила] → Факты/сущности → Компактное хранилище
                                                    ↓
                              AI запрашивает: "что знаем про X?"
                                                    ↓
                              Ответ: 50 токенов структурированных фактов
```

## Почему это важно

Если решить проблему сжатия знаний до минимального контекста без потери смысла — это проект на $100M+ инвестиций. Продукт вокруг — это уже тестирование гипотезы.

---

## Направления исследования

### 1. Extractive Summarization Pipeline

**Идея:** Дешёвая модель (Haiku/Gemini Flash) выделяет ключевые факты.

```
Документ (10K токенов)
    → Haiku выделяет ключевые факты (500 токенов)
    → В контекст идёт только summary
```

**Экономика:**
- Haiku: $0.25/1M input, $1.25/1M output
- Sonnet: $3/1M input, $15/1M output
- Сжатие 10K → 500 токенов = экономия 95% на inference

**Вопросы:**
- [ ] Какой prompt даёт лучшее сжатие без потери смысла?
- [ ] Можно ли обойтись без LLM (правила, NLP)?
- [ ] Как валидировать качество сжатия?

---

### 2. Hierarchical Knowledge Index

**Идея:** Пирамида абстракций — AI спускается только в нужные ветки.

```
Level 3: Граф понятий (ключевые сущности + связи)     ~100 токенов
Level 2: Summary кластеров документов                  ~1K токенов
Level 1: Summary каждого документа                     ~10K токенов
Level 0: Полные документы (не грузим в контекст)       ~1M токенов
```

**Алгоритм поиска:**
1. AI смотрит Level 3 — понимает структуру знаний
2. Выбирает релевантные кластеры → Level 2
3. Выбирает конкретные документы → Level 1
4. Только если нужны детали → Level 0 (редко)

**Вопросы:**
- [ ] Как автоматически строить иерархию?
- [ ] Как кластеризовать документы?
- [ ] Какой уровень гранулярности оптимален?

---

### 3. Structured Facts Extraction

**Идея:** Вместо прозы — структурированные факты.

```yaml
# Было: 500 токенов прозы про сервис
# Стало: 50 токенов структуры

entity: "Сервис X"
type: service
facts:
  - owner: "team-platform"
  - deploys_to: ["prod-1", "prod-2"]
  - depends_on: ["postgres", "redis"]
  - last_incident: "2025-01-10"
  - runbook: "link"
relations:
  - calls: ["Сервис Y", "Сервис Z"]
  - called_by: ["API Gateway"]
```

**Преимущества:**
- 10x сжатие без потери фактов
- Машиночитаемый формат
- Можно строить граф связей

**Вопросы:**
- [ ] Какие типы сущностей универсальны?
- [ ] Как извлекать факты автоматически?
- [ ] Как хранить и запрашивать?

---

### 4. Query-Aware Compression

**Идея:** Сжимать не заранее, а под конкретный вопрос.

```
Вопрос: "Как деплоить сервис X?"
    ↓
Найдены документы: [doc1, doc2, doc3] — 15K токенов
    ↓
Дешёвая модель фильтрует под вопрос
    ↓
В контекст: 1.5K токенов (10% от оригинала)
```

**Trade-off:**
- Дополнительный вызов дешёвой модели
- Но экономия на дорогой модели больше

**Вопросы:**
- [ ] Какой порог релевантности?
- [ ] Можно ли без LLM (TF-IDF, BM25, embeddings)?
- [ ] Как не потерять важный контекст?

---

### 5. Entity-Centric Knowledge Graph

**Идея:** Хранить не документы, а сущности с атрибутами.

```
┌─────────────┐     calls      ┌─────────────┐
│  Сервис X   │───────────────▶│  Сервис Y   │
│  owner: A   │                │  owner: B   │
│  status: ok │                │  status: ok │
└─────────────┘                └─────────────┘
       │
       │ has_incident
       ▼
┌─────────────┐
│ Incident 42 │
│ date: 01-10 │
│ resolved: ✓ │
└─────────────┘
```

**Запрос:** "Что знаем про Сервис X?"
**Ответ:** Граф с 3 сущностями, ~100 токенов вместо 3 документов по 2K токенов.

**Вопросы:**
- [ ] Как автоматически извлекать сущности?
- [ ] Какая схема графа универсальна?
- [ ] Как обновлять граф инкрементально?

---

## Эксперименты

### Эксперимент 1: Baseline

**Цель:** Измерить текущие затраты на типовые вопросы.

| Вопрос | Документов | Токенов в контекст | Стоимость |
|--------|------------|-------------------|-----------|
| TBD    | TBD        | TBD               | TBD       |

### Эксперимент 2: Extractive Summary

**Цель:** Сравнить качество ответов при сжатии через Haiku.

| Сжатие | Токенов | Качество ответа | Стоимость |
|--------|---------|-----------------|-----------|
| 0%     | TBD     | baseline        | TBD       |
| 50%    | TBD     | TBD             | TBD       |
| 90%    | TBD     | TBD             | TBD       |

---

## Открытые вопросы

1. **Универсальность vs специализация** — делать общий инструмент или под конкретный домен (код, wiki, эксперименты)?

2. **Online vs offline compression** — сжимать заранее или на лету?

3. **Lossy vs lossless** — допустима ли потеря информации? Какой процент?

4. **Метрики качества** — как измерить, что сжатие не испортило ответы?

5. **Интеграция** — CLI? API? Плагин для Claude Code?

---

## Следующие шаги

- [ ] Собрать датасет: типовые вопросы + документы + эталонные ответы
- [ ] Реализовать baseline: RAG без сжатия, измерить токены
- [ ] Реализовать extractive summary через Haiku
- [ ] Сравнить качество и стоимость
- [ ] Итерировать

---

## Ссылки

- [ast-index](https://github.com/defendend/Claude-ast-index-search) — вдохновение по быстрому структурному поиску
- TBD: papers, конкуренты, примеры
